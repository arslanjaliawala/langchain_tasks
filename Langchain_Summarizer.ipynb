{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179ace81-fd62-484c-bbb7-0326bba98f2d",
   "metadata": {},
   "source": [
    "Thought Process\n",
    "While researching a solution for this particular task, i found three solutions provided by Langchain library out of the box\n",
    "1) Stuf Chain\n",
    "2) Map-Reduce\n",
    "3) Refine\n",
    "\n",
    "Based on my research all these solutions are impactful for a smaller size of documents than the book provided. For the number of tokens generated in the book Stuff Chain wouldnt work as the Context window of gpt 4o (the model i used for both the solutions) is 128,000 tokens i.e. less than the tokens in our book. \n",
    "For Map-Reduce my research pointed out that Map-Reduce can work as a solution however both Map-Reduce and refine are not a very cost effective solution. To keep the context of the task as close to the real world situations as possible i opted for another strategy very briefly described in the following bullets:\n",
    "1) Split the documents in separate chunks\n",
    "2) Generate vector embeddings of those chunks\n",
    "3) Perform clustering over the vectors\n",
    "4) Select the vectors which are the closest to the centroids of the clusters\n",
    "5) Now on the selected vectors perform map reduce i.e. pass the text respective to the vector to the llm instructing the LLM to summarise the text\n",
    "6) Then pass all the summaries to the LLM asking it to derive a final summary as per requirements.\n",
    "7) Save the Final Summary to a pdf file.\n",
    "\n",
    "Post Script: These two links helped me navigate and reach this approach: \n",
    "1) https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/5%20Levels%20Of%20Summarization%20-%20Novice%20To%20Expert.ipynb\n",
    "2) https://pashpashpash.substack.com/p/tackling-the-challenge-of-document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b82fd0-e523-411c-9d8a-e3d0f1a6cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70389dc-c1e0-45f9-ae68-27cd281fa4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9edb8c71-9160-4e8d-811d-8fa8e8581add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing my model\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da4bc4d-daa2-4068-928c-b57d159d9dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore programmer.\", response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 19, 'total_tokens': 23}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_abc28019ad', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-f1654f2b-caf2-472e-8fa1-f3c5dcf32fef-0', usage_metadata={'input_tokens': 19, 'output_tokens': 4, 'total_tokens': 23})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a sample task to model for smoke testing\n",
    "message = HumanMessage(\n",
    "    content=\"Translate this sentence from English to French. I love programming.\"\n",
    ")\n",
    "model.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499172ee-7d2f-49f0-8365-c4021671f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used later on to code to clean the pdf textbook\n",
    "import re\n",
    "def clean_text(text):\n",
    "   # Remove the specific phrase 'Free eBooks at Planet eBook.com' and surrounding whitespace\n",
    "   cleaned_text = re.sub(r'\\s*Free eBooks at Planet eBook\\.com\\s*', '', text, flags=re.DOTALL)\n",
    "   # Remove extra spaces\n",
    "   cleaned_text = re.sub(r'\\n+', ' ', cleaned_text)\n",
    "   # Remove any appearance of multiple spaces with a single space \n",
    "   cleaned_text = re.sub(r' +', ' ', cleaned_text)\n",
    "   # Remove non-printable characters, optionally preceded by 'Crime and Punishment'\n",
    "   cleaned_text = re.sub(r'(Crime and Punishment )?[\\x00-\\x1F]', '', cleaned_text)\n",
    "   # Replace newline characters with spaces\n",
    "   cleaned_text = re.sub(r'\\s*-\\s*', '-', cleaned_text) # Retain hyphens and remove spaces around them\n",
    "    # Remove leading and trailing spaces\n",
    "   cleaned_text = cleaned_text.strip()\n",
    "   return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843b3eb2-66de-45ff-b631-5a908edf95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the book\n",
    "loader = PyPDFLoader(\"C:/Users/stech/Downloads/crime-and-punishment.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# Cut out the irrelevant open and closing parts\n",
    "pages = pages[6:743]\n",
    "\n",
    "# Combine the pages, and replace the tabs with spaces\n",
    "text = \"\"\n",
    "\n",
    "for page in pages:\n",
    "    text += page.page_content\n",
    "    \n",
    "text = text.replace('\\t', ' ')\n",
    "clean_text=clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b3e19d2-86cb-45d9-a507-c714c9009005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book has 267185 tokens in it\n"
     ]
    }
   ],
   "source": [
    "#check how many num_tokens are does the book translate to so that a cost effective decision can be reached over to use map_reduce approach or the current approach\n",
    "num_tokens = model.get_num_tokens(clean_text)\n",
    "\n",
    "print (f\"This book has {num_tokens} tokens in it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d195df-365b-4bdf-9610-53aa52d7fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the book into chunks of 20,000 with an overlap of 5,000 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=20000, chunk_overlap=5000)\n",
    "#prepare documents from the cleaned text\n",
    "docs = text_splitter.create_documents([clean_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d238c0-9460-4f77-b38d-65ea13c90975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many documents are generated\n",
    "num_documents = len(docs)\n",
    "print (f\"Now our book is split up into {num_documents} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1f0eb-c72f-4d0d-a901-849ba5840565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the embedding model\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"ada-002\",\n",
    "    openai_api_version=\"2024-06-01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c7eba-314e-4d78-856a-f4af24ce32a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate embeddings of the documents. The separate commands and sleep() function is used because a single call was leading to more calls to Azure Open AI than the specific call_rate resulting in an error. \n",
    "import time\n",
    "vectors = embeddings.embed_documents([x.page_content for x in docs[0:24]])\n",
    "time.sleep(5)\n",
    "vectors += embeddings.embed_documents([x.page_content for x in docs[24:48]])\n",
    "time.sleep(5)\n",
    "vectors += embeddings.embed_documents([x.page_content for x in docs[48:72]])\n",
    "time.sleep(5)\n",
    "vectors += embeddings.embed_documents([x.page_content for x in docs[72:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bea41-0471-4d87-bdf8-bec551b0e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e72e9-001f-4f88-ab8c-e35ef88b4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49503330-273c-4c7b-8735-ac9aca2e72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'embeddings' is a list or array of 1536-dimensional embeddings\n",
    "# Choose the number of clusters, these clusters are the most important sections of the book from which we will drive the summary of our book.\n",
    "\n",
    "num_clusters = 15\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029ae99-5e1f-4519-a008-19126248ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d53df7-5878-42b1-a52f-afe26cd96a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from warnings import simplefilter\n",
    "\n",
    "# Filter out FutureWarnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Perform t-SNE and reduce to 2 dimensions\n",
    "vectorsarray = np.array(vectors)\n",
    "n_samples = vectorsarray.shape[0]\n",
    "# Set perplexity to a value less than the number of samples\n",
    "perplexity = min(30, n_samples - 1)\n",
    "tsne = TSNE(n_components=2, random_state=42,perplexity=perplexity)\n",
    "reduced_data_tsne = tsne.fit_transform(vectorsarray)\n",
    "\n",
    "# Plot the reduced data\n",
    "plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1], c=kmeans.labels_)\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Book Embeddings Clustered')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded0175-50eb-498d-908a-5d4351c58093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest embeddings to the centroids\n",
    "# Create an empty list that will hold your closest points\n",
    "closest_indices = []\n",
    "\n",
    "# Iterate over the number of clusters you have to extract only those vectors which are the closest to the centroids\n",
    "for i in range(num_clusters):\n",
    "    \n",
    "    # Get the list of distances from that particular cluster center\n",
    "    distances = np.linalg.norm(vectors - kmeans.cluster_centers_[i], axis=1)\n",
    "    \n",
    "    # Find the list position of the closest one (using argmin to find the smallest distance)\n",
    "    closest_index = np.argmin(distances)\n",
    "    \n",
    "    # Append that position to your closest indices list\n",
    "    closest_indices.append(closest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c903ea-1b00-4f43-b315-356b2dd7df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the list\n",
    "selected_indices = sorted(closest_indices)\n",
    "selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39cca8-6d8c-4f42-b73c-bc075e48a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "#prompt to generate a summary of the selected vectors\n",
    "map_prompt = \"\"\"\n",
    "You will be given a single passage of a book. This section will be enclosed in triple backticks (```)\n",
    "Your goal is to give a summary of this section so that a reader will have a full understanding of what happened.\n",
    "Your response should be at least three paragraphs and fully encompass what was said in the passage.\n",
    "\n",
    "```{text}```\n",
    "FULL SUMMARY:\n",
    "\"\"\"\n",
    "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a574ac-ce39-4668-a0b4-9bfe74f63061",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_chain = load_summarize_chain(llm=model,\n",
    "                             chain_type=\"stuff\",\n",
    "                             prompt=map_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a09cd6-0ec2-465e-8f50-75af373a74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_docs = [docs[doc] for doc in selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a1dd6-8525-4b49-9675-e88130d23802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to hold your summaries\n",
    "summary_list = []\n",
    "\n",
    "# Loop through a range of the lenght of your selected docs\n",
    "for i, doc in enumerate(selected_docs):\n",
    "    \n",
    "    # Go get a summary of the chunk\n",
    "    chunk_summary = map_chain.run([doc])\n",
    "    \n",
    "    # Append that summary to your list\n",
    "    summary_list.append(chunk_summary)\n",
    "    \n",
    "    print (f\"Summary #{i} (chunk #{selected_indices[i]}) - Preview: {chunk_summary[:250]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37204239-1e17-4bf6-a6bf-470329b0d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = \"\\n\".join(summary_list)\n",
    "\n",
    "# Convert it back to a document\n",
    "summaries = Document(page_content=summaries)\n",
    "\n",
    "print (f\"Your total summary has {model.get_num_tokens(summaries.page_content)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f30fd-c073-461f-a670-1b8e751287c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt to generate a summary from the summaries we have generated from the previous prompt\n",
    "combine_prompt = \"\"\"\n",
    "You will be given a series of summaries from a book. The summaries will be enclosed in triple backticks (```)\n",
    "Your goal is to give a verbose summary of approximately 6000 words of what happened in the story.\n",
    "The reader should be able to grasp what happened in the book.\n",
    "\n",
    "```{text}```\n",
    "VERBOSE SUMMARY:\n",
    "\"\"\"\n",
    "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ab20f-e1e3-4678-9cba-aa4df8247f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_chain = load_summarize_chain(llm=model,\n",
    "                             chain_type=\"stuff\",\n",
    "                             prompt=combine_prompt_template,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f692c9-19e5-4455-bda0-f1285daeb7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = reduce_chain.run([summaries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97febe12-20df-4715-9023-4d7bf6c5e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8c6e7-8a91-498b-bc50-a6c243b4aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "pdf.multi_cell(0, 10, output.encode('latin-1', 'replace').decode('latin-1'))\n",
    "pdf_output_path = \"C:/Users/stech/Downloads/Summary_15.pdf\"\n",
    "pdf.output(pdf_output_path)\n",
    "print(f\"PDF saved successfully at {pdf_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e967a-c006-4e1f-b498-baa45c6c8c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
